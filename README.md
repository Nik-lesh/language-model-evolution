# Language Model Evolution: RNN ‚Üí LSTM ‚Üí Transformer

Comparing three generations of language models trained on financial text, demonstrating architectural improvements and the critical impact of tokenization strategy and dataset size.

## üéØ Project Overview

Built and trained three neural network architectures from scratch on two tokenization approaches:

- **Simple RNN** - Baseline sequential model
- **LSTM** - Improved with memory gates
- **Transformer** - State-of-the-art attention mechanism

**Tokenization Strategies:**

- Character-level (113 vocab)
- Word-level (9,751 vocab)

**Goal:** Compare architectures and understand when each excels

## üèÜ Results

### Character-Level Tokenization (640KB, 652K characters)

| Model       | Parameters | Val Loss   | Train Time | Text Quality     |
| ----------- | ---------- | ---------- | ---------- | ---------------- |
| Simple RNN  | 274K       | 1.6482     | 3.8 min    | Poor (gibberish) |
| **LSTM** ü•á | 3.8M       | **1.4711** | 39.5 min   | **Excellent**    |
| Transformer | 3.2M       | 1.5523     | 103 min    | Undertrained     |

### Word-Level Tokenization (640KB, 132K words)

| Model              | Parameters | Val Loss   | Train Time | Text Quality                 |
| ------------------ | ---------- | ---------- | ---------- | ---------------------------- |
| LSTM               | 3.8M       | 6.4072     | 15.7 min   | Good grammar, weak semantics |
| **Transformer** ü•á | 3.2M       | **6.2291** | 25.8 min   | Similar, slightly better     |

### Sample Generations: "Money is..."

**Character-Level LSTM (Best Overall):**

```
"Money is always right to seek by less than investments. When the
result is that the poor and the drivers that are high-specialized..."
```

‚úÖ Real words, financial vocabulary, semantic coherence

**Word-Level Transformer:**

```
"money is all, the road and white kahneman hurried off the second
investor, but if you want to be so losing money at some three..."
```

‚úÖ Perfect grammar, learned names (Kahneman)  
‚ùå Semantically incoherent (insufficient data)

## üîç Key Findings

### 1. Character-Level: LSTM Wins

- **Best validation loss:** 1.4711
- **Best text quality** for small datasets
- **Optimal for:** <1MB text, character-level modeling
- Generated financial concepts: "$17,000 a month", "retirement traders"

### 2. Word-Level Requires More Data

- **Both models struggled** (6.2-6.4 loss vs 1.5 loss)
- **Vocabulary too sparse:** 9,751 words / 132K total = 13.6 samples per word
- **Transformer beats LSTM** at word-level (6.23 vs 6.41)
- **Conclusion:** Need 10-20x more data for word-level to work

### 3. Critical Insight: Data Size Matters More Than Architecture

- Small dataset (640KB) ‚Üí Character-level dominates
- Large dataset (5-10MB) ‚Üí Word-level should dominate
- **Architecture choice depends on data availability**

### 4. Tokenization Strategy Impact

| Aspect                        | Character-Level | Word-Level           |
| ----------------------------- | --------------- | -------------------- |
| **Vocabulary**                | 113             | 9,751                |
| **Samples per token**         | 5,775           | 13.6                 |
| **Best for**                  | Small datasets  | Large datasets       |
| **Training speed**            | Slower          | Faster               |
| **Text quality (small data)** | Better          | Worse                |
| **Text quality (large data)** | Good            | Excellent (expected) |

## üöÄ Quick Start

```bash
# Setup
git clone https://github.com/YOUR_USERNAME/language-model-evolution.git
cd language-model-evolution
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Character-level training
python src/train.py lstm           # 40 min
python src/train.py transformer    # 100 min

# Word-level training
python src/scripts/prepare_word_level_data.py
python src/train_word_level.py lstm        # 16 min
python src/train_word_level.py transformer # 26 min

# Compare results
python src/compare_models.py
python src/generate_samples.py
```

## üìÅ Project Structure

```
language-model-evolution/
‚îú‚îÄ‚îÄ data/                          # Training data (not in git)
‚îÇ   ‚îú‚îÄ‚îÄ books/                    # Source books by category
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gutenberg/            # 60 original classics (34 MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gutenberg_expanded/   # 169 additional books (73 MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wikipedia/            # 127 articles (4.6 MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ academic_text/        # 10 extracted papers (1.3 MB)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ old_books/            # Original 2 books (0.6 MB)
‚îÇ   ‚îú‚îÄ‚îÄ mega_corpus.txt           # Combined corpus (103 MB, 18.3M words)
‚îÇ   ‚îú‚îÄ‚îÄ dataset.pkl               # Character-level dataset (652K chars)
‚îÇ   ‚îî‚îÄ‚îÄ word_dataset.pkl          # Word-level dataset (small corpus)
‚îÇ
‚îú‚îÄ‚îÄ pdfs/                          # Original PDFs (not in git)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Neural network architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_rnn.py         # Simple RNN implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm.py               # LSTM with gates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer.py        # Multi-head attention
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                  # Data processing utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_data/        # Data collection scripts (local only)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Data preparation utilities
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prepare_data.py              # Character-level prep
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ prepare_word_level_data.py   # Word-level prep
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analyze/                  # Analysis and visualization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis_corpus.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_training.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_word_level.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compare_models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate_samples.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ train/                    # Training scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Character-level training
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_word_level.py   # Word-level training
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ clean_corpus.py           # Text cleaning utility
‚îÇ
‚îú‚îÄ‚îÄ results/                      # Training visualizations (in git)
‚îÇ   ‚îú‚îÄ‚îÄ word_level/               # Word-level experiment results
‚îÇ   ‚îú‚îÄ‚îÄ simple_rnn_training_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ lstm_training_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ transformer_training_curve.png
‚îÇ   ‚îî‚îÄ‚îÄ rnn_vs_lstm_comparison.png
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                  # Trained models (not in git)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üìä Dataset Summary

### Current Dataset (103 MB - Phase 2 Complete)

- **Total Size:** 103.14 MB
- **Total Words:** 18,330,423 (138x increase from original)
- **Total Sources:** 379 books and articles
- **Vocabulary:** ~20,000 unique words
- **Samples per Word:** ~916 (vs 13.6 in small dataset)

### Sources Breakdown

| Source             | Files   | Size       | Content                          |
| ------------------ | ------- | ---------- | -------------------------------- |
| Gutenberg Original | 60      | 31 MB      | Classic finance books            |
| Gutenberg Expanded | 169     | 66 MB      | Economics, investment, business  |
| Wikipedia          | 127     | 4.3 MB     | Finance/economics articles       |
| Academic Papers    | 10      | 1.3 MB     | Research papers (arXiv)          |
| **Total**          | **379** | **103 MB** | **Comprehensive finance corpus** |

### Categories Covered

- üìö Economics theory and history
- üíπ Investment and trading strategies
- üè¢ Business and entrepreneurship
- üí∞ Personal finance and wealth building
- üèõÔ∏è Banking and monetary systems
- üìä Accounting and financial management
- üåç International trade and economics
- üëî Labor and industrial relations

## üöÄ Usage

### Prepare Datasets

**Character-level (small corpus):**

```bash
python src/scripts/utils/prepare_data.py
```

**Word-level (mega corpus):**

```bash
python src/scripts/utils/prepare_word_level_data.py data/mega_corpus.txt
```

### Train Models

**Character-level:**

```bash
python src/train/train.py rnn
python src/train/train.py lstm
python src/train/train.py transformer
```

**Word-level (on mega corpus):**

```bash
python src/train/train_word_level.py lstm
python src/train/train_word_level.py transformer
```

### Analyze Results

```bash
python src/analyze/analyze_training.py lstm
python src/analyze/compare_models.py
python src/analyze/generate_samples.py
```

## üìà Expected Results with Mega Corpus

| Model                | Small Dataset | Mega Dataset      | Improvement        |
| -------------------- | ------------- | ----------------- | ------------------ |
| **Char LSTM**        | 1.47 loss     | N/A               | Baseline           |
| **Word LSTM**        | 6.41 loss     | ~1.8-2.2 loss     | 3-4x better        |
| **Word Transformer** | 6.23 loss     | **~1.2-1.8 loss** | **3-5x better** üéØ |

**With 18.3M words, Transformer should dominate!**

## üî¨ Technical Details

**Character-Level Architectures:**

- **RNN:** Embedding(113, 128) ‚Üí RNN(128, 256, 2 layers) ‚Üí Linear(256, 113)
- **LSTM:** Embedding(113, 256) ‚Üí LSTM(256, 512, 2 layers) ‚Üí Linear(512, 113)
- **Transformer:** Embedding(113, 256) ‚Üí 4-layer (8 heads) ‚Üí Linear(256, 113)

**Word-Level Architectures:**

- **Vocabulary:** 9,751 words (top 10K, 100% coverage)
- **Sequence length:** 50 words (vs 100 chars)
- **Same model architectures**, different vocab size

**Training Details:**

- Optimizer: Adam
- Batch size: 64 (char), 32 (word)
- Loss: Cross-entropy
- Hardware: CPU (Apple Silicon)

## üìä Visualizations

See `results/` for:

- Training curves (all models)
- Character vs word-level comparison
- Text generation examples

## üöÄ Next Steps: Phase 2 (In Progress)

### Current Status

‚úÖ Proved word-level needs more data  
‚úÖ Transformer beats LSTM at word-level  
üîÑ Collecting 50-100 finance books

### Phase 2: Massive Dataset Expansion

**Goal:** 5-10MB corpus (10-20x larger)

**Books Sources:**

- Project Gutenberg economics classics (13+ books)
- Federal Reserve publications
- IMF/World Bank reports
- Modern finance bestsellers (library/purchase)

**Expected Impact:**

- Word vocabulary: 15-20K words
- Total words: 2-5M (vs current 132K)
- Word-level loss: < 2.0 (vs current 6.2)
- **Transformer should dominate**

### Phase 3: GPU Training

- Platform: Google Colab (free T4 GPU)
- Speed: 10-50x faster training
- Enables: Larger models, rapid experimentation

### Phase 4: Production Financial Advisor

- GPT-style chat interface
- FastAPI + React deployment
- Real-time financial advice generation

## üìö Key Learnings

1. **Data size trumps architecture** - Match your approach to your data
2. **Character-level works great for small datasets** - Don't underestimate simplicity
3. **Word-level requires 10-20x more data** - Vocabulary sparsity is critical
4. **Transformers need proper conditions** - Not universally superior
5. **LSTM remains powerful** - Still competitive for many tasks
6. **Tokenization matters as much as architecture** - Choose wisely

## üõ†Ô∏è Tech Stack

- PyTorch 2.0
- NumPy, Matplotlib
- pdfplumber (PDF extraction)
- tqdm (progress tracking)

## üìä Comparison Summary

| Scenario                            | Winner                 | Reason                     |
| ----------------------------------- | ---------------------- | -------------------------- |
| **Small dataset + character-level** | LSTM                   | Optimal samples per token  |
| **Small dataset + word-level**      | Transformer            | But both perform poorly    |
| **Large dataset + word-level**      | Transformer (expected) | Attention shines with data |

## üìÑ License

MIT License

## üôè Acknowledgments

- Dataset: Finance books (Rich Dad Poor Dad, Psychology of Money)
- Inspired by: Evolution of NLP architectures
- Built for: Understanding when each architecture excels

---

**Current Status:**  
‚úÖ **Phase 1 Complete** - Word-level validation  
üîÑ **Phase 2 In Progress** - Dataset expansion (targeting 50-100 books)  
‚è≥ **Phase 3 Planned** - GPU training  
‚è≥ **Phase 4 Planned** - Production deployment

**Key Takeaway:** Small dataset (640KB) ‚Üí Character-level LSTM wins (1.47 loss)  
**Next Goal:** Large dataset (5-10MB) ‚Üí Word-level Transformer should dominate

**Last Updated:** November 9, 2025
