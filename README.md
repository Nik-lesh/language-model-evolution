# Language Model Evolution: RNN â†’ LSTM â†’ Transformer

Comparing three generations of language models trained on the same financial text corpus.

## ğŸ¯ Project Goal

Train and compare three different neural network architectures for character-level language modeling:

1. **Simple RNN** (baseline) - Shows fundamental sequence modeling
2. **LSTM** (improved) - Demonstrates handling of long-term dependencies
3. **Transformer** (state-of-the-art) - Modern attention-based architecture

All models trained on finance/investment books to learn domain-specific language.

## ğŸ“Š Dataset

- **Domain:** Finance and Investment
- **Size:** ~640 KB of text (~652,809 characters)
- **Vocabulary:** 113 unique characters
- **Books:** 2 finance books focused on money, wealth, and investing
- **Preprocessing:** Character-level tokenization

## ğŸ—ï¸ Project Structure

```
language-model-evolution/
â”œâ”€â”€ data/                          # Training data (not in git)
â”‚   â”œâ”€â”€ training_corpus.txt       # Raw combined text
â”‚   â”œâ”€â”€ training_corpus_clean.txt # Cleaned text
â”‚   â””â”€â”€ dataset.pkl               # Processed dataset
â”‚
â”œâ”€â”€ pdfs/                          # Original PDF books (not in git)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scripts/                   # Data processing scripts
â”‚   â”‚   â”œâ”€â”€ extract_from_pdf.py   # PDF â†’ text extraction
â”‚   â”‚   â”œâ”€â”€ clean_corpus.py       # Text cleaning
â”‚   â”‚   â”œâ”€â”€ analysis_corpus.py    # Dataset statistics
â”‚   â”‚   â””â”€â”€ prepare_data.py       # Create training sequences
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # Model architectures
â”‚   â”‚   â”œâ”€â”€ simple_rnn.py         # Basic RNN
â”‚   â”‚   â”œâ”€â”€ lstm.py               # LSTM (coming soon)
â”‚   â”‚   â””â”€â”€ transformer.py        # Transformer (coming soon)
â”‚   â”‚
â”‚   â””â”€â”€ train.py                   # Training script (coming soon)
â”‚
â”œâ”€â”€ checkpoints/                   # Saved models (not in git)
â”œâ”€â”€ results/                       # Training results (not in git)
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for analysis
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Setup

### 1. Clone the repository

```bash
git clone https://github.com/Nik.lesh/language-model-evolution.git
cd language-model-evolution
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Prepare your data

Since the PDFs are copyrighted, you'll need to provide your own:

- Place PDF files in `pdfs/` folder
- Run data extraction: `python src/scripts/extract_from_pdf.py`
- Clean and prepare: `python src/scripts/prepare_data.py`

## ğŸ“¦ Dependencies

```
torch>=2.0.0
numpy>=1.24.0
pdfplumber>=0.11.0
PyPDF2>=3.0.0
matplotlib>=3.7.0
jupyter>=1.0.0
tqdm>=4.65.0
```

## ğŸ“ Models

### Simple RNN

- **Parameters:** ~XXX,XXX (to be filled after training)
- **Architecture:** Embedding â†’ 2-layer RNN â†’ Linear
- **Purpose:** Baseline model showing fundamental sequence modeling

### LSTM (Coming Soon)

- **Architecture:** Embedding â†’ 2-layer LSTM â†’ Linear
- **Improvement:** Better at learning long-term dependencies via gates

### Transformer (Coming Soon)

- **Architecture:** Embedding â†’ Multi-head Attention â†’ Feed-forward â†’ Linear
- **Improvement:** Parallel processing and attention mechanism

## ğŸ“ˆ Training (Coming Soon)

```bash
# Train Simple RNN
python src/train.py --model rnn --epochs 50 --batch-size 64

# Train LSTM
python src/train.py --model lstm --epochs 50 --batch-size 64

# Train Transformer
python src/train.py --model transformer --epochs 50 --batch-size 32
```

## ğŸ“Š Results (Coming Soon)

Comparison metrics:

- Training loss curves
- Validation loss
- Text generation quality
- Training time
- Model size
- Perplexity

## ğŸ¯ Sample Outputs (Coming Soon)

Examples of text generated by each model starting with "Money is..."

## ğŸ” Analysis (Coming Soon)

Jupyter notebooks for:

- Data exploration
- Training visualization
- Model comparison
- Text generation demos

## ğŸ“ Key Learnings

This project demonstrates:

- Evolution of sequence modeling architectures
- Importance of gates (LSTM) for long-term dependencies
- Power of attention mechanisms (Transformer)
- Character-level vs word-level modeling
- Practical ML project workflow: data â†’ model â†’ training â†’ evaluation

## ğŸ¤ Contributing

This is a learning project. Feel free to fork and experiment!

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- Dataset: Finance and investment books
- Inspired by: The evolution of NLP architectures
- Built for: Understanding deep learning fundamentals

---

**Status:** ğŸš§ In Progress - Currently implementing RNN baseline

Last Updated: November 3, 2025
